from flask import Flask, jsonify, send_file
import os

app = Flask(__name__)

# ğŸ“Œ Ù†Ù…Ø§ÛŒØ´ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø³Ø§ÛŒØª (index.html Ø¯Ø± Ù‡Ù…Ø§Ù† Ù¾ÙˆØ´Ù‡)
@app.route('/')
def home():
    return send_file("index.html")

# ğŸ“Œ API Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§ÛŒØª
@app.route('/optimize')
def optimize():
    return jsonify({"message": "Website optimization started!"})

# ğŸ“Œ Ø³Ø±Ùˆ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© (CSS, JS, Images) Ø§Ø² Ù¾ÙˆØ´Ù‡ Ø§ØµÙ„ÛŒ
@app.route('/<path:filename>')
def serve_static(filename):
    if os.path.exists(filename):
        return send_file(filename)
    return "File not found", 404

# ğŸ“Œ Ø­Ù„ Ù…Ø´Ú©Ù„ escape sequence Ø¯Ø± gzip
@app.route('/compress')
def compress():
    os.system("find . -type f -name '*.css' -exec gzip {} ';'")
    os.system("find . -type f -name '*.js' -exec gzip {} ';'")
    return jsonify({"message": "Compression completed!"})

if __name__ == '__main__':
    app.run(debug=True)

from flask import Flask, jsonify
import threading

app = Flask(__name__)

class AIWebsiteOptimizer:
    def __init__(self):
        pass

    def start_monitoring(self):
        self.schedule_task()

    def schedule_task(self):
        threading.Timer(60.0, self.run_tasks).start()

    def run_tasks(self):
        self.scan_website()
        self.optimize_performance()
        self.enhance_security()
        self.adjust_ui_for_devices()
        self.schedule_task()

    def scan_website(self):
        print("Scanning website for issues...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ù† Ø³Ø§ÛŒØª Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø´Ú©Ù„Ø§Øª

    def optimize_performance(self):
        print("Optimizing website speed and efficiency...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª

    def enhance_security(self):
        print("Enhancing website security and threat detection...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ ØªÙ‚ÙˆÛŒØª Ø§Ù…Ù†ÛŒØª Ø³Ø§ÛŒØª

    def adjust_ui_for_devices(self):
        print("Adjusting UI for all screen sizes and devices...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø·Ø±Ø§Ø­ÛŒ Ø³Ø§ÛŒØª Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§

# Ø§ÛŒØ¬Ø§Ø¯ API Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø§Ø² Ø³Ø§ÛŒØª
@app.route('/optimize', methods=['GET'])
def optimize():
    optimizer = AIWebsiteOptimizer()
    optimizer.run_tasks()
    return jsonify({"status": "success", "message": "Optimization tasks started successfully!"})

if __name__ == '__main__':
    optimizer = AIWebsiteOptimizer()
    optimizer.start_monitoring()
    app.run(debug=True, port=5000)

from flask import Flask
import threading

app = Flask(__name__)

class AIWebsiteOptimizer:
    def __init__(self):
        pass

    def start_monitoring(self):
        self.schedule_task()

    def schedule_task(self):
        threading.Timer(60.0, self.run_tasks).start()

    def run_tasks(self):
        self.scan_website()
        self.optimize_performance()
        self.enhance_security()
        self.adjust_ui_for_devices()
        self.schedule_task()

    def scan_website(self):
        print("Scanning website for issues...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ù† Ø³Ø§ÛŒØª Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø´Ú©Ù„Ø§Øª

    def optimize_performance(self):
        print("Optimizing website speed and efficiency...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª

    def enhance_security(self):
        print("Enhancing website security and threat detection...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ ØªÙ‚ÙˆÛŒØª Ø§Ù…Ù†ÛŒØª Ø³Ø§ÛŒØª

    def adjust_ui_for_devices(self):
        print("Adjusting UI for all screen sizes and devices...")
        # Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø·Ø±Ø§Ø­ÛŒ Ø³Ø§ÛŒØª Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§

@app.route('/')
def home():
    return "Welcome to the AI Website Optimizer!"

if __name__ == '__main__':
    optimizer = AIWebsiteOptimizer()
    optimizer.start_monitoring()
    app.run(debug=True)

import time
import threading

class AIWebsiteOptimizer:
    def __init__(self):
        pass

    def start_monitoring(self):
        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø¨Ù‡ ØµÙˆØ±Øª Ø²Ù…Ø§Ù†Ø¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ù‡Ø± 60 Ø«Ø§Ù†ÛŒÙ‡
        self.schedule_task()

    def schedule_task(self):
        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø¯Ø± Ù‡Ø± 60 Ø«Ø§Ù†ÛŒÙ‡
        threading.Timer(60.0, self.run_tasks).start()

    def run_tasks(self):
        self.scan_website()
        self.optimize_performance()
        self.enhance_security()
        self.adjust_ui_for_devices()
        # Ø¨Ø¹Ø¯ Ø§Ø² 60 Ø«Ø§Ù†ÛŒÙ‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ÙˆØ¸Ø§ÛŒÙ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        self.schedule_task()

    def scan_website(self):
        print("Scanning website for issues...")
        # ØªØ­Ù„ÛŒÙ„ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± AI Ø¨Ø±Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª ÙˆØ¨Ø³Ø§ÛŒØªØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ØŒ Ùˆ SEO

    def optimize_performance(self):
        print("Optimizing website speed and efficiency...")
        # Ú©Ø´ÛŒÙ†Ú¯ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± AIØŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ± Ùˆ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø±

    def enhance_security(self):
        print("Enhancing website security and threat detection...")
        # Ø¯ÛŒÙˆØ§Ø± Ø¢ØªØ´ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± AIØŒ Ø­ÙØ§Ø¸Øª Ø§Ø² DDoS Ùˆ Ø§Ø³Ú©Ù† Ø¢Ø³ÛŒØ¨â€ŒÙ¾Ø°ÛŒØ±ÛŒâ€ŒÙ‡Ø§

    def adjust_ui_for_devices(self):
        print("Adjusting UI for all screen sizes and devices...")
        # Ø·Ø±Ø§Ø­ÛŒ ÙˆØ§Ú©Ù†Ø´â€ŒÚ¯Ø±Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú†ÛŒØ¯Ù…Ø§Ù† Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± AI

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
optimizer = AIWebsiteOptimizer()
optimizer.start_monitoring()

from flask import Flask, send_from_directory, render_template_string

app = Flask(__name__)

# Ù…Ø³ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ index.html
@app.route('/')
def home():
    return render_template_string(open('index.html').read())  # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ§ÛŒÙ„ HTML Ø§Ø² Ù¾ÙˆØ´Ù‡ Ø§ØµÙ„ÛŒ

# Ù†Ù…Ø§ÛŒØ´ ÙØ§ÛŒÙ„ CSS
@app.route('/style.css')
def css():
    return send_from_directory('.', 'style.css', mimetype='text/css')

# Ù†Ù…Ø§ÛŒØ´ ÙØ§ÛŒÙ„ JavaScript
@app.route('/script.js')
def js():
    return send_from_directory('.', 'script.js', mimetype='application/javascript')

# Ù†Ù…Ø§ÛŒØ´ favicon.ico
@app.route('/favicon.ico')
def favicon():
    return send_from_directory('.', 'favicon.ico', mimetype='image/x-icon')

if __name__ == "__main__":
    app.run(debug=True)
    
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/get_data')
def get_data():
    return jsonify({"message": "Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆØ± Flask Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯!"})

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, request, jsonify
import sqlite3

app = Flask(__name__)

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø§Ø³Ø® Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
def get_response(user_message):
    conn = sqlite3.connect('chat_db.db')
    c = conn.cursor()
    c.execute('SELECT response FROM chat_responses WHERE question = ?', (user_message,))
    result = c.fetchone()
    conn.close()
    return result[0] if result else "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…ØŒ Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†."

# Ø±ÙˆØª Ú†Øª
@app.route('/chat', methods=['POST'])
def chat():
    user_message = request.json['message']
    response = get_response(user_message)
    return jsonify({"response": response})

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, request, jsonify

app = Flask(__name__)

# Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§
chat_db = {
    "Ø³Ù„Ø§Ù…": "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…ØŸ",
    "Ú†Ù‡ Ø®Ø¨Ø±ØŸ": "Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ù†Ø¯Ø§Ø±Ù…! ÙˆÙ„ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…."
    
}

@app.route('/chat', methods=['POST'])
def chat():
    user_message = request.json['message']
    response = chat_db.get(user_message, "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…ØŒ Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†.")
    return jsonify({"response": response})

if __name__ == '__main__':
    app.run(debug=True)
import sqlite3

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø¬Ø¯ÙˆÙ„
conn = sqlite3.connect('chat_db.db')
c = conn.cursor()

# Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§
c.execute('''
    CREATE TABLE IF NOT EXISTS chat_responses (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        question TEXT NOT NULL,
        response TEXT NOT NULL
    )
''')

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§
c.execute('INSERT INTO chat_responses (question, response) VALUES (?, ?)', ('Ø³Ù„Ø§Ù…', 'Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…ØŸ'))
c.execute('INSERT INTO chat_responses (question, response) VALUES (?, ?)', ('Ú†Ù‡ Ø®Ø¨Ø±ØŸ', 'Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ù†Ø¯Ø§Ø±Ù…!'))

# Ø°Ø®ÛŒØ±Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ùˆ Ø¨Ø³ØªÙ† Ø§Ø±ØªØ¨Ø§Ø·
conn.commit()
conn.close()
from flask import Flask, request, jsonify
import sqlite3

app = Flask(__name__)

def get_response(user_message):
    conn = sqlite3.connect("chatbot.db")
    cursor = conn.cursor()
    cursor.execute("SELECT response FROM responses WHERE question = ?", (user_message,))
    result = cursor.fetchone()
    conn.close()
    return result[0] if result else "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…! Ù„Ø·ÙØ§Ù‹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ù¾Ø±Ø³."

@app.route("/chat", methods=["POST"])
def chat():
    data = request.json
    user_message = data.get("message", "").strip()
    response = get_response(user_message)
    return jsonify({"response": response})

if __name__ == "__main__":
    app.run(debug=True)
from flask import Flask, render_template, send_from_directory

app = Flask(__name__)

# Ù…Ø³ÛŒØ± Ø¨Ø±Ø§ÛŒ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø³Ø§ÛŒØª
@app.route('/')
def home():
    return render_template('index.html')  # ØµÙØ­Ù‡ HTML

# Ù…Ø³ÛŒØ± Ø¨Ø±Ø§ÛŒ favicon (Ø¢ÛŒÚ©ÙˆÙ† Ø³Ø§ÛŒØª)
@app.route('/favicon.ico')
def favicon():
    return send_from_directory('.', 'favicon.ico')  # Ø¢ÛŒÚ©ÙˆÙ† Ø³Ø§ÛŒØª Ø¯Ø± Ù‡Ù…Ø§Ù† Ù¾ÙˆØ´Ù‡

# Ù…Ø³ÛŒØ± Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© (CSS Ùˆ JS)
@app.route('/style.css')
def style():
    return send_from_directory('.', 'style.css')  # ÙØ§ÛŒÙ„ CSS Ø¯Ø± Ù‡Ù…Ø§Ù† Ù¾ÙˆØ´Ù‡

@app.route('/js.js')
def script():
    return send_from_directory('.', 'js.js')  # ÙØ§ÛŒÙ„ JavaScript Ø¯Ø± Ù‡Ù…Ø§Ù† Ù¾ÙˆØ´Ù‡

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, render_template, send_from_directory
import requests
from bs4 import BeautifulSoup

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ø±Ø¹Øª Ø³Ø§ÛŒØª Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
def analyze_website(url):
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø¯ÛŒØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    images = soup.find_all('img')
    for img in images:
        img_url = img.get('src')
        if img_url:
            print(f"Ø¨Ø±Ø±Ø³ÛŒ ØªØµÙˆÛŒØ±: {img_url}")
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ± (Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„)
            # Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ØªØµØ§ÙˆÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

    # ØªØ­Ù„ÛŒÙ„ Ù…Ø´Ú©Ù„Ø§Øª Ù…ØªØ§ ØªÚ¯â€ŒÙ‡Ø§ (SEO)
    title_tag = soup.find('title')
    if title_tag:
        print(f"Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡: {title_tag.text}")
    else:
        print("Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")

# ØªØ­Ù„ÛŒÙ„ Ø³Ø§ÛŒØª Ø´Ù…Ø§
analyze_website('https://yourgamingnews.com')


app = Flask(__name__)

# ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ
@app.route('/')
def home():
    return render_template('index.html')  # ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù¾ÙˆØ´Ù‡ templates Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯

# Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ø§ÛŒ favicon
@app.route('/favicon.ico')
def favicon():
    return send_from_directory('.', 'favicon.ico')  # Ø¢ÛŒÚ©ÙˆÙ† Ú©Ù‡ Ø¯Ø± Ø±ÛŒØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/analyze', methods=['GET'])
def analyze():
    url = request.args.get('url')
    if not url:
        return jsonify({"error": "No URL provided"}), 400
    return jsonify({"message": f"Analyzing {url}..."}), 200

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
from flask import Flask, render_template

app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/data')  # Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ùˆ Ø¨Ø¯ÙˆÙ† ÙØ§ØµÙ„Ù‡ Ø§Ø¶Ø§ÙÛŒ
def data():
    return "Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"

def check_website(url):
    try:
        response = requests.get(url, timeout=5)
        status_code = response.status_code
        
        if status_code == 200:
            print(f"âœ… Ø³Ø§ÛŒØª {url} Ø³Ø§Ù„Ù… Ø§Ø³Øª!")
        elif status_code == 404:
            print(f"âš ï¸ Ø®Ø·Ø§! ØµÙØ­Ù‡ {url} ÛŒØ§ÙØª Ù†Ø´Ø¯ (404).")
        elif status_code == 500:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ± Ø¯Ø± Ø³Ø§ÛŒØª {url} (500).")
        else:
            print(f"âš ï¸ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {status_code}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ Ø¯Ø§Ø®Ù„ ØµÙØ­Ù‡
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a['href'] for a in soup.find_all('a', href=True)]
        
        for link in links:
            if link.startswith("http"):
                try:
                    link_response = requests.get(link, timeout=3)
                    if link_response.status_code == 404:
                        print(f"ğŸš¨ Ù„ÛŒÙ†Ú© Ø®Ø±Ø§Ø¨: {link}")
                except requests.exceptions.RequestException:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©: {link}")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

import requests
from bs4 import BeautifulSoup
from flask import Flask, render_template

app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/data')
def data():
    return "Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"

def check_website(url):
    try:
        response = requests.get(url, timeout=5)
        status_code = response.status_code
        
        if status_code == 200:
            print(f"âœ… Ø³Ø§ÛŒØª {url} Ø³Ø§Ù„Ù… Ø§Ø³Øª!")
        elif status_code == 404:
            print(f"âš ï¸ Ø®Ø·Ø§! ØµÙØ­Ù‡ {url} ÛŒØ§ÙØª Ù†Ø´Ø¯ (404).")
        elif status_code == 500:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ± Ø¯Ø± Ø³Ø§ÛŒØª {url} (500).")
        else:
            print(f"âš ï¸ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {status_code}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ Ø¯Ø§Ø®Ù„ ØµÙØ­Ù‡
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a['href'] for a in soup.find_all('a', href=True)]
        
        for link in links:
            if link.startswith("http"):
                try:
                    link_response = requests.get(link, timeout=3)
                    if link_response.status_code == 404:
                        print(f"ğŸš¨ Ù„ÛŒÙ†Ú© Ø®Ø±Ø§Ø¨: {link}")
                except requests.exceptions.RequestException:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©: {link}")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

import requests
from bs4 import BeautifulSoup
from flask import Flask, render_template
import time
import openai

openai.api_key = "YOUR_API_KEY"


app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/data')
def data():
    return "Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"

if __name__ == '__main__':
    app.run(debug=True)


import subprocess

def generate_text(prompt, model="mistral", max_tokens=100):
    """ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ù…Ø­Ù„ÛŒ Ø¨Ø§ ollama Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API """
    command = ["ollama", "run", model, prompt, "--max-tokens", str(max_tokens)]
    result = subprocess.run(command, capture_output=True, text=True)
    return result.stdout.strip()

# ØªØ³Øª Ù…Ø¯Ù„
print(generate_text("Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú†ÛŒØ³ØªØŸ"))


app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/data')
def data():
    return "Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
def check_website(url):
    try:
        response = requests.get(url, timeout=5)
        status_code = response.status_code
        
        if status_code == 200:
            print(f"âœ… Ø³Ø§ÛŒØª {url} Ø³Ø§Ù„Ù… Ø§Ø³Øª!")
        elif status_code == 404:
            print(f"âš ï¸ Ø®Ø·Ø§! ØµÙØ­Ù‡ {url} ÛŒØ§ÙØª Ù†Ø´Ø¯ (404).")
        elif status_code == 500:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ± Ø¯Ø± Ø³Ø§ÛŒØª {url} (500).")
        else:
            print(f"âš ï¸ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {status_code}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ Ø¯Ø§Ø®Ù„ ØµÙØ­Ù‡
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a['href'] for a in soup.find_all('a', href=True)]
        
        for link in links:
            if link.startswith("http"):
                try:
                    link_response = requests.get(link, timeout=3)
                    if link_response.status_code == 404:
                        print(f"ğŸš¨ Ù„ÛŒÙ†Ú© Ø®Ø±Ø§Ø¨: {link}")
                except requests.exceptions.RequestException:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©: {link}")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øª Ù„ÙˆØ¯ Ø³Ø§ÛŒØª
def check_speed(url):
    start_time = time.time()
    try:
        response = requests.get(url, timeout=5)
        load_time = time.time() - start_time
        print(f"â³ Ø³Ø±Ø¹Øª Ù„ÙˆØ¯ Ø³Ø§ÛŒØª {url}: {load_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        
        if load_time > 3:
            print("âš ï¸ Ø³Ø§ÛŒØª Ú©Ù†Ø¯ Ø§Ø³Øª! Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ± Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN")
    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

# ØªØ³Øª ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØª
check_website("https://example.com")
check_speed("https://example.com")

if __name__ == '__main__':
    app.run(debug=True)


def generate_blog_post(topic):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯."},
            {"role": "user", "content": f"ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ {topic} Ø¨Ù†ÙˆÛŒØ³."}
        ]
    )
    return response["choices"][0]["message"]["content"]

print(generate_blog_post("Ø¨Ù‡ØªØ±ÛŒÙ† ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ 2025"))
from flask import Flask, request

app = Flask(__name__)

blocked_ips = set()

@app.before_request
def block_bad_ips():
    ip = request.remote_addr
    if ip in blocked_ips:
        return "â›” Ø¯Ø³ØªØ±Ø³ÛŒ Ø´Ù…Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª!", 403

@app.route('/')
def home():
    return "ğŸš€ Ø³Ø§ÛŒØª Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ø§Ø³Øª!"

@app.route('/block_ip/<ip>')
def block_ip(ip):
    blocked_ips.add(ip)
    return f"ğŸ”´ IP {ip} Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯!"

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup

def check_website(url):
    try:
        response = requests.get(url, timeout=5)
        status_code = response.status_code
        
        if status_code == 200:
            print(f"âœ… Ø³Ø§ÛŒØª {url} Ø³Ø§Ù„Ù… Ø§Ø³Øª!")
        elif status_code == 404:
            print(f"âš ï¸ Ø®Ø·Ø§! ØµÙØ­Ù‡ {url} ÛŒØ§ÙØª Ù†Ø´Ø¯ (404).")
        elif status_code == 500:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ± Ø¯Ø± Ø³Ø§ÛŒØª {url} (500).")
        else:
            print(f"âš ï¸ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {status_code}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a['href'] for a in soup.find_all('a', href=True)]
        
        for link in links:
            if link.startswith("http"):
                try:
                    link_response = requests.get(link, timeout=3)
                    if link_response.status_code == 404:
                        print(f"ğŸš¨ Ù„ÛŒÙ†Ú© Ø®Ø±Ø§Ø¨: {link}")
                except requests.exceptions.RequestException:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©: {link}")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

# ØªØ³Øª Ø¨Ø§ Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª
check_website("https://example.com")
import time
import requests

def check_speed(url):
    start_time = time.time()
    try:
        response = requests.get(url, timeout=5)
        load_time = time.time() - start_time
        print(f"â³ Ø³Ø±Ø¹Øª Ù„ÙˆØ¯ Ø³Ø§ÛŒØª {url}: {load_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        
        if load_time > 3:
            print("âš ï¸ Ø³Ø§ÛŒØª Ú©Ù†Ø¯ Ø§Ø³Øª! Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±ØŒ Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN")
    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

check_speed("https://example.com")
from selenium import webdriver

def test_responsive(url):
    driver = webdriver.Chrome()

    screen_sizes = [(1920, 1080), (1366, 768), (1024, 768), (768, 1024), (375, 667)]  
    for width, height in screen_sizes:
        driver.set_window_size(width, height)
        driver.get(url)
        print(f"âœ… Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØª Ø¯Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡ {width}x{height}")

    driver.quit()

test_responsive("https://example.com")
from flask import Flask, request

app = Flask(__name__)

blocked_ips = set()

@app.before_request
def block_bad_ips():
    ip = request.remote_addr
    if ip in blocked_ips:
        return "â›” Ø¯Ø³ØªØ±Ø³ÛŒ Ø´Ù…Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª!", 403

@app.route('/')
def home():
    return "ğŸš€ Ø³Ø§ÛŒØª Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ø§Ø³Øª!"

@app.route('/block_ip/<ip>')
def block_ip(ip):
    blocked_ips.add(ip)
    return f"ğŸ”´ IP {ip} Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯!"

if __name__ == '__main__':
    app.run(debug=True)
import openai

openai.api_key = "YOUR_API_KEY"

def generate_blog_post(topic):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯."},
            {"role": "user", "content": f"ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ {topic} Ø¨Ù†ÙˆÛŒØ³."}
        ]
    )
    return response["choices"][0]["message"]["content"]

print(generate_blog_post("Ø¨Ù‡ØªØ±ÛŒÙ† ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ 2025"))
import requests
from bs4 import BeautifulSoup

def check_website(url):
    try:
        response = requests.get(url, timeout=5)
        status_code = response.status_code

        if status_code == 200:
            print(f"âœ… Ø³Ø§ÛŒØª {url} Ø³Ø§Ù„Ù… Ø§Ø³Øª!")
        elif status_code in [404, 500, 403]:
            print(f"âš ï¸ Ø®Ø·Ø§! ØµÙØ­Ù‡ {url} Ø§Ø±ÙˆØ± {status_code} Ø¯Ø§Ø±Ø¯.")

        soup = BeautifulSoup(response.text, "html.parser")
        links = [a['href'] for a in soup.find_all('a', href=True)]

        for link in links:
            if link.startswith("http"):
                try:
                    if requests.get(link, timeout=3).status_code == 404:
                        print(f"ğŸš¨ Ù„ÛŒÙ†Ú© Ø®Ø±Ø§Ø¨: {link}")
                except requests.exceptions.RequestException:
                    print(f"âŒ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©: {link}")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

check_website("https://example.com")
import time
import requests

def check_speed(url):
    start_time = time.time()
    try:
        response = requests.get(url, timeout=5)
        load_time = time.time() - start_time
        print(f"â³ Ø³Ø±Ø¹Øª Ù„ÙˆØ¯ Ø³Ø§ÛŒØª {url}: {load_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

        if load_time > 3:
            print("âš ï¸ Ø³Ø§ÛŒØª Ú©Ù†Ø¯ Ø§Ø³Øª! Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±ØŒ Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN")

    except requests.exceptions.RequestException:
        print(f"âŒ Ø³Ø§ÛŒØª {url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª!")

check_speed("https://example.com")
from flask import Flask, request

app = Flask(__name__)
blocked_ips = set()

@app.before_request
def block_bad_ips():
    ip = request.remote_addr
    if ip in blocked_ips:
        return "â›” Ø¯Ø³ØªØ±Ø³ÛŒ Ø´Ù…Ø§ Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª!", 403

@app.route('/block_ip/<ip>')
def block_ip(ip):
    blocked_ips.add(ip)
    return f"ğŸ”´ IP {ip} Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯!"

if __name__ == '__main__':
    app.run(debug=True)
from bs4 import BeautifulSoup
import requests

def seo_analysis(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    title = soup.title.string if soup.title else "âŒ Ø¹Ù†ÙˆØ§Ù† Ù†Ø¯Ø§Ø±Ø¯"
    description = soup.find("meta", attrs={"name": "description"})
    keywords = soup.find("meta", attrs={"name": "keywords"})

    print(f"ğŸ” Ø¹Ù†ÙˆØ§Ù†: {title}")
    print(f"ğŸ“Œ ØªÙˆØ¶ÛŒØ­Ø§Øª: {description['content'] if description else 'âŒ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ø¯Ø§Ø±Ø¯'}")
    print(f"ğŸ·ï¸ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ: {keywords['content'] if keywords else 'âŒ Ù†Ø¯Ø§Ø±Ø¯'}")

seo_analysis("https://example.com")
import openai

openai.api_key = "YOUR_API_KEY"

def generate_blog_post(topic):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯."},
            {"role": "user", "content": f"ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ {topic} Ø¨Ù†ÙˆÛŒØ³."}
        ]
    )
    return response["choices"][0]["message"]["content"]

print(generate_blog_post("ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Û²Û°Û²Ûµ"))
import pandas as pd

data = pd.read_csv("user_activity.csv")
most_viewed_pages = data['page'].value_counts()
print("ğŸ“Š ØµÙØ­Ø§Øª Ù¾Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯:")
print(most_viewed_pages)
from flask import Flask, render_template, request

app = Flask(__name__)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/process", methods=["POST"])
def process():
    user_input = request.form["user_input"]
    response = f"ğŸ¤– Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {user_input.upper()}"  # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯
    return render_template("index.html", response=response)

if __name__ == "__main__":
    app.run(debug=True)
from flask import Flask, render_template, request

app = Flask(__name__)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/process", methods=["POST"])
def process():
    user_input = request.form["user_input"]
    response = f"ğŸ¤– Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {user_input.upper()}"  # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯
    return render_template("index.html", response=response)

if __name__ == "__main__":
    app.run(debug=True)
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/api/process", methods=["POST"])
def api_process():
    data = request.json
    user_input = data.get("text", "")
    response = {"result": f"ğŸ” ØªØ­Ù„ÛŒÙ„ Ø´Ø¯: {user_input.upper()}"}
    return jsonify(response)

if __name__ == "__main__":
    app.run(debug=True)
from flask import Flask, render_template, request

app = Flask(__name__)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/process", methods=["POST"])
def process():
    user_input = request.form["user_input"]
    response = f"ğŸ¤– Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {user_input.upper()}"  # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯
    return render_template("index.html", response=response)

if __name__ == "__main__":
    app.run(debug=True)
    import requests
from bs4 import BeautifulSoup
import re
import time
import logging
import json
from urllib.parse import urljoin

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AIWebsiteManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.device_compatibility_issues = []
        self.content_redundancy_issues = []

    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)  # Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        self.errors.append(f'Broken link: {link} (Status {res.status_code})')
                except requests.exceptions.RequestException:
                    self.errors.append(f'Unreachable link: {link}')
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øª Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                self.performance_issues.append(f'Page load time is slow: {load_time:.2f} seconds')
        except requests.exceptions.RequestException as e:
            self.errors.append(f'Performance issue: {e}')

    def check_seo(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØ§ Ø¯ÛŒØªØ§ Ùˆ Ø³Ø¦Ùˆ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('title').text if soup.find('title') else 'No Title'
            description = soup.find('meta', attrs={'name': 'description'})
            description = description['content'] if description else 'No Description'
            if len(title) < 10 or len(description) < 50:
                self.errors.append('SEO Warning: Title or Description is too short')
        except Exception as e:
            logging.error(f'Error checking SEO: {e}')
    
    def check_device_compatibility(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù """
        device_resolutions = [(1920, 1080), (1366, 768), (768, 1024), (375, 667)]  # Desktop, Tablet, Mobile
        for width, height in device_resolutions:
            response = requests.get(self.url, headers={'User-Agent': f'Mozilla/5.0 (Width: {width}, Height: {height})'})
            if response.status_code != 200:
                self.device_compatibility_issues.append(f'Compatibility issue detected for {width}x{height}')
    
    def check_content_redundancy(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            content_list = [p.text.strip() for p in paragraphs if p.text.strip()]
            duplicate_content = set([content for content in content_list if content_list.count(content) > 1])
            if duplicate_content:
                self.content_redundancy_issues.append(f'Redundant content detected: {duplicate_content}')
        except Exception as e:
            logging.error(f'Error checking content redundancy: {e}')

    def report_issues(self):
        """ Ú†Ø§Ù¾ Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ """
        issues = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Device Compatibility Issues': self.device_compatibility_issues,
            'Content Redundancy Issues': self.content_redundancy_issues
        }
        logging.info(json.dumps(issues, indent=4))

    def run_diagnostics(self):
        logging.info(f'Running diagnostics on {self.url} ...')
        self.check_broken_links()
        self.check_performance()
        self.check_seo()
        self.check_device_compatibility()
        self.check_content_redundancy()
        self.report_issues()

# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == "__main__":
    website_url = "https://example.com"  # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª Ø®ÙˆØ¯ Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯
    manager = AIWebsiteManager(website_url)
    manager.run_diagnostics()

    import requests
from bs4 import BeautifulSoup
import re
import time
import logging
import json
from urllib.parse import urljoin
import threading

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AIWebsiteManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.device_compatibility_issues = []
        self.content_redundancy_issues = []
        self.security_issues = []

    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        self.errors.append(f'Broken link: {link} (Status {res.status_code})')
                except requests.exceptions.RequestException:
                    self.errors.append(f'Unreachable link: {link}')
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øª Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                self.performance_issues.append(f'Page load time is slow: {load_time:.2f} seconds')
        except requests.exceptions.RequestException as e:
            self.errors.append(f'Performance issue: {e}')

    def check_seo(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØ§ Ø¯ÛŒØªØ§ Ùˆ Ø³Ø¦Ùˆ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('title').text if soup.find('title') else 'No Title'
            description = soup.find('meta', attrs={'name': 'description'})
            description = description['content'] if description else 'No Description'
            if len(title) < 10 or len(description) < 50:
                self.errors.append('SEO Warning: Title or Description is too short')
        except Exception as e:
            logging.error(f'Error checking SEO: {e}')
    
    def check_device_compatibility(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù """
        device_resolutions = [(1920, 1080), (1366, 768), (768, 1024), (375, 667)]
        for width, height in device_resolutions:
            try:
                response = requests.get(self.url, headers={'User-Agent': f'Mozilla/5.0 (Width: {width}, Height: {height})'})
                if response.status_code != 200:
                    self.device_compatibility_issues.append(f'Compatibility issue detected for {width}x{height}')
            except Exception as e:
                logging.error(f'Error checking device compatibility: {e}')
    
    def check_content_redundancy(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            content_list = [p.text.strip() for p in paragraphs if p.text.strip()]
            duplicate_content = set([content for content in content_list if content_list.count(content) > 1])
            if duplicate_content:
                self.content_redundancy_issues.append(f'Redundant content detected: {duplicate_content}')
        except Exception as e:
            logging.error(f'Error checking content redundancy: {e}')
    
    def check_security_issues(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                self.security_issues.append('Security Warning: X-Frame-Options header missing')
            if 'Content-Security-Policy' not in response.headers:
                self.security_issues.append('Security Warning: Content-Security-Policy header missing')
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def report_issues(self):
        """ Ú†Ø§Ù¾ Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ """
        issues = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Device Compatibility Issues': self.device_compatibility_issues,
            'Content Redundancy Issues': self.content_redundancy_issues,
            'Security Issues': self.security_issues
        }
        logging.info(json.dumps(issues, indent=4))

    def run_diagnostics(self):
        logging.info(f'Running diagnostics on {self.url} ...')
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_seo),
            threading.Thread(target=self.check_device_compatibility),
            threading.Thread(target=self.check_content_redundancy),
            threading.Thread(target=self.check_security_issues)
        ]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        self.report_issues()

# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == "__main__":
    website_url = "https://example.com"  # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª Ø®ÙˆØ¯ Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯
    manager = AIWebsiteManager(website_url)
    manager.run_diagnostics()
    import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import nltk
from urllib.parse import urljoin
from flask import Flask, request, jsonify

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        self.errors.append(f'Broken link: {link} (Status {res.status_code})')
                except requests.exceptions.RequestException:
                    self.errors.append(f'Unreachable link: {link}')
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                self.performance_issues.append(f'Page load time is slow: {load_time:.2f} seconds')
        except requests.exceptions.RequestException as e:
            self.errors.append(f'Performance issue: {e}')
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                self.security_issues.append('Security Warning: X-Frame-Options header missing')
            if 'Content-Security-Policy' not in response.headers:
                self.security_issues.append('Security Warning: Content-Security-Policy header missing')
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import nltk
from urllib.parse import urljoin
from flask import Flask, request, jsonify
import sqlite3
import hashlib
import os

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analysis_pipeline = pipeline("sentiment-analysis")
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def analyze_sentiment(self):
        """ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø³Ø§ÛŒØª """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            text_content = " ".join([p.text for p in soup.find_all('p')])
            sentiment = self.sentiment_analysis_pipeline(text_content[:1000])
            return sentiment
        except Exception as e:
            logging.error(f'Error analyzing sentiment: {e}')
            return []
    
    def detect_spam_comments(self):
        """ ØªØ´Ø®ÛŒØµ Ù†Ø¸Ø±Ø§Øª Ø§Ø³Ù¾Ù… Ø¯Ø± Ø³Ø§ÛŒØª """
        spam_keywords = ["free", "win money", "click here", "subscribe", "limited offer"]
        response = requests.get(self.url)
        soup = BeautifulSoup(response.text, 'html.parser')
        comments = [comment.text for comment in soup.find_all('p')]
        spam_comments = [c for c in comments if any(keyword in c.lower() for keyword in spam_keywords)]
        return spam_comments
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        sentiment_result = self.analyze_sentiment()
        spam_comments = self.detect_spam_comments()
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data,
            'User Sentiment': sentiment_result,
            'Spam Comments Detected': spam_comments
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
    import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analysis_pipeline = pipeline("sentiment-analysis")
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def analyze_trending_topics(self):
        """ ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº Ø¯Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        vectorizer = TfidfVectorizer(stop_words='english')
        X = vectorizer.fit_transform(self.news_data)
        kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
        topics = kmeans.cluster_centers_.argsort()[:, ::-1]
        terms = vectorizer.get_feature_names_out()
        trending_topics = [terms[i] for i in topics[0][:5]]
        return trending_topics
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        trending_topics = self.analyze_trending_topics()
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data,
            'Trending Topics': trending_topics
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analysis_pipeline = pipeline("sentiment-analysis")
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def analyze_trending_topics(self):
        """ ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº Ø¯Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        vectorizer = TfidfVectorizer(stop_words='english')
        X = vectorizer.fit_transform(self.news_data)
        kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
        topics = kmeans.cluster_centers_.argsort()[:, ::-1]
        terms = vectorizer.get_feature_names_out()
        trending_topics = [terms[i] for i in topics[0][:5]]
        return trending_topics
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        trending_topics = self.analyze_trending_topics()
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data,
            'Trending Topics': trending_topics
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from flask_cors import CORS
import random
import matplotlib.pyplot as plt

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)
CORS(app)  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† CORS Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analysis_pipeline = pipeline("sentiment-analysis")
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                self.news_data.extend(headlines[:5])
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def visualize_data(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¨ØµØ±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ """
        labels = ['Errors', 'Performance Issues', 'Security Issues']
        values = [len(self.errors), len(self.performance_issues), len(self.security_issues)]
        
        plt.figure(figsize=(6, 6))
        plt.bar(labels, values, color=['red', 'blue', 'orange'])
        plt.xlabel('Categories')
        plt.ylabel('Count')
        plt.title('Website Analysis Summary')
        plt.show()
    
    def report_status(self):
        """ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        report = {
            'Errors': self.errors,
            'Performance Issues': self.performance_issues,
            'Security Issues': self.security_issues,
            'Latest Gaming News': self.news_data,
        }
        return json.dumps(report, indent=4)
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return self.report_status()

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(json.loads(report))

if __name__ == '__main__':
    app.run(debug=True)
os.system(r"find /var/www/html -type f -name '*.css' -exec gzip {} \;")
os.system(r"find /var/www/html -type f -name '*.js' -exec gzip {} \;")
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from flask_cors import CORS
import random
import matplotlib.pyplot as plt
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)
CORS(app)  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† CORS Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS news (id INTEGER PRIMARY KEY, headline TEXT, sentiment TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                
                for headline in headlines[:5]:
                    sentiment = self.sentiment_analyzer.polarity_scores(headline)
                    sentiment_label = "Positive" if sentiment['compound'] > 0 else "Negative" if sentiment['compound'] < 0 else "Neutral"
                    self.cursor.execute('INSERT INTO news (headline, sentiment) VALUES (?, ?)', (headline, sentiment_label))
                self.conn.commit()
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return {"message": "Analysis completed"}

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(report)

if __name__ == '__main__':
    app.run(debug=True)
import requests
from bs4 import BeautifulSoup
import time
import logging
import json
import threading
import sqlite3
import hashlib
import os
import torch
import transformers
from transformers import pipeline
from flask import Flask, request, jsonify
from urllib.parse import urljoin
from datetime import datetime
import re
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from flask_cors import CORS
import random
import matplotlib.pyplot as plt
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Flask Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§ÛŒØª
app = Flask(__name__)
CORS(app)  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† CORS Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯

class GamingAIManager:
    def __init__(self, url):
        self.url = url
        self.errors = []
        self.performance_issues = []
        self.security_issues = []
        self.news_data = []
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.setup_database()
    
    def setup_database(self):
        """ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª """
        self.conn = sqlite3.connect('site_data.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, issue TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS news (id INTEGER PRIMARY KEY, headline TEXT, sentiment TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def log_issue(self, issue):
        """ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ """
        self.cursor.execute('INSERT INTO logs (issue) VALUES (?)', (issue,))
        self.conn.commit()
    
    def check_broken_links(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨ """
        try:
            response = requests.get(self.url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [a.get('href') for a in soup.find_all('a', href=True)]
            
            for link in links:
                if not link.startswith('http'):
                    link = urljoin(self.url, link)
                try:
                    res = requests.head(link, timeout=5)
                    if res.status_code >= 400:
                        issue = f'Broken link: {link} (Status {res.status_code})'
                        self.errors.append(issue)
                        self.log_issue(issue)
                except requests.exceptions.RequestException:
                    issue = f'Unreachable link: {link}'
                    self.errors.append(issue)
                    self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking broken links: {e}')
    
    def check_performance(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§ÛŒØª """
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            load_time = time.time() - start_time
            if load_time > 3:
                issue = f'Page load time is slow: {load_time:.2f} seconds'
                self.performance_issues.append(issue)
                self.log_issue(issue)
        except requests.exceptions.RequestException as e:
            issue = f'Performance issue: {e}'
            self.errors.append(issue)
            self.log_issue(issue)
    
    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù…Ù†ÛŒØªÛŒ """
        try:
            response = requests.get(self.url)
            if 'X-Frame-Options' not in response.headers:
                issue = 'Security Warning: X-Frame-Options header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
            if 'Content-Security-Policy' not in response.headers:
                issue = 'Security Warning: Content-Security-Policy header missing'
                self.security_issues.append(issue)
                self.log_issue(issue)
        except Exception as e:
            logging.error(f'Error checking security: {e}')
    
    def fetch_latest_gaming_news(self):
        """ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª """
        news_sources = ["https://www.pcgamer.com/news/", "https://kotaku.com/c/gaming"]
        
        for source in news_sources:
            try:
                response = requests.get(source)
                soup = BeautifulSoup(response.text, 'html.parser')
                headlines = [h.text.strip() for h in soup.find_all('h2')]
                
                for headline in headlines[:5]:
                    sentiment = self.sentiment_analyzer.polarity_scores(headline)
                    sentiment_label = "Positive" if sentiment['compound'] > 0 else "Negative" if sentiment['compound'] < 0 else "Neutral"
                    self.cursor.execute('INSERT INTO news (headline, sentiment) VALUES (?, ?)', (headline, sentiment_label))
                self.conn.commit()
            except Exception as e:
                logging.error(f'Error fetching news from {source}: {e}')
    
    def run_analysis(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù† """
        threads = [
            threading.Thread(target=self.check_broken_links),
            threading.Thread(target=self.check_performance),
            threading.Thread(target=self.check_security),
            threading.Thread(target=self.fetch_latest_gaming_news)
        ]
        
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        
        return {"message": "Analysis completed"}

# API Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª
@app.route('/analyze', methods=['GET'])
def analyze_website():
    website_url = request.args.get('url')
    if not website_url:
        return jsonify({"error": "Please provide a website URL"}), 400
    
    manager = GamingAIManager(website_url)
    report = manager.run_analysis()
    return jsonify(report)

if __name__ == '__main__':
    app.run(debug=True)
import requests
import json
import os
from time import sleep

# Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ùˆ Ø­Ù„ Ù…Ø´Ú©Ù„Ø§Øª Ø³Ø§ÛŒØª
class SiteAI:
    def __init__(self, site_url):
        self.site_url = site_url
        self.issue_log = []

    def check_site_status(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª """
        try:
            response = requests.get(self.site_url)
            if response.status_code != 200:
                self.issue_log.append(f"Ù…Ø´Ú©Ù„ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø§ÛŒØª: {self.site_url}")
                self.fix_issue('site_down')
            else:
                print(f"Ø³Ø§ÛŒØª {self.site_url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª.")
        except requests.exceptions.RequestException as e:
            self.issue_log.append(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø§ÛŒØª: {str(e)}")
            self.fix_issue('site_down')

    def fix_issue(self, issue_type):
        """ Ø­Ù„ Ù…Ø´Ú©Ù„Ø§Øª Ø®ÙˆØ¯Ú©Ø§Ø± """
        if issue_type == 'site_down':
            print("Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø§ÛŒØª...")
            # Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ Ø³Ø§ÛŒØª
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ú©Ø¯ØŒ Ø§Ù‚Ø¯Ø§Ù…Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø±ÛŒØ³ØªØ§Ø±Øª Ø³Ø±ÙˆØ± ÛŒØ§ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª DNS Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
            sleep(3)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø²Ù…Ø§Ù† Ø±ÙØ¹ Ù…Ø´Ú©Ù„
            print(f"Ù…Ø´Ú©Ù„ {issue_type} Ø±ÙØ¹ Ø´Ø¯.")
            self.issue_log.append(f"Ù…Ø´Ú©Ù„ {issue_type} Ø±ÙØ¹ Ø´Ø¯.")

    def analyze_user_behavior(self):
        """ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù† """
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…Ø´Ú©Ù„Ø§Øª ÛŒØ§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.
        pass

    def apply_updates(self):
        """ Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ """
        print("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯...")
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø³ÛŒØ³ØªÙ…
        pass

# Ø§Ø¬Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØª
site_ai = SiteAI('https://example.com')
site_ai.check_site_status()
site_ai.apply_updates()
import requests
import os
import time
import logging
import subprocess
import psutil
import json
from bs4 import BeautifulSoup
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡
SITE_URL = "https://example.com"  # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª
LOG_FILE = "site_ai_log.txt"
SECURITY_CHECKS = ["sql_injection", "xss", "ddos_protection"]
DB_BACKUP_PATH = "/backups/site_backup.sql"

# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

class SmartSiteAI:
    def __init__(self, site_url):
        self.site_url = site_url
        self.issue_log = []

    def check_site_status(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ø§ÛŒØª Ùˆ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ø¢Ù† """
        try:
            response = requests.get(self.site_url)
            if response.status_code != 200:
                self.log_issue(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø§ÛŒØª: {self.site_url}")
                self.fix_issue('site_down')
            else:
                print(f"âœ… Ø³Ø§ÛŒØª {self.site_url} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª.")
        except requests.exceptions.RequestException as e:
            self.log_issue(f"âŒ Ø®Ø·Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø§ÛŒØª: {str(e)}")
            self.fix_issue('site_down')

    def analyze_html(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø± HTML Ø³Ø§ÛŒØª """
        response = requests.get(self.site_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        errors = []
        if not soup.find("title"):
            errors.append("ğŸš¨ Ø³Ø§ÛŒØª Ø¹Ù†ÙˆØ§Ù† (title) Ù†Ø¯Ø§Ø±Ø¯.")
        if not soup.find("meta", {"name": "description"}):
            errors.append("âš ï¸ Ø³Ø§ÛŒØª ØªÙˆØ¶ÛŒØ­Ø§Øª Ù…ØªØ§ Ù†Ø¯Ø§Ø±Ø¯.")
        if errors:
            self.log_issue("\n".join(errors))

    def fix_issue(self, issue_type):
        """ Ø­Ù„ Ù…Ø´Ú©Ù„Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± """
        if issue_type == 'site_down':
            print("ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø§ÛŒØª...")
            self.restart_server()

    def restart_server(self):
        """ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø³Ø±ÙˆØ± Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² """
        os.system("systemctl restart apache2")
        logging.info("ğŸ” Ø³Ø±ÙˆØ± Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯.")

    def check_security(self):
        """ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ù†ÛŒØª Ø³Ø§ÛŒØª Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø­Ù…Ù„Ø§Øª SQL Injection Ùˆ XSS """
        vulnerabilities = []
        for check in SECURITY_CHECKS:
            if check == "sql_injection":
                response = requests.get(f"{self.site_url}/?id=1' OR '1'='1")
                if "error" in response.text.lower():
                    vulnerabilities.append("âš ï¸ Ø³Ø§ÛŒØª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± SQL Injection Ø¢Ø³ÛŒØ¨â€ŒÙ¾Ø°ÛŒØ± Ø¨Ø§Ø´Ø¯.")
            if check == "xss":
                response = requests.get(f"{self.site_url}/?search=<script>alert('XSS')</script>")
                if "<script>" in response.text:
                    vulnerabilities.append("ğŸš¨ Ø³Ø§ÛŒØª Ù†Ø³Ø¨Øª Ø¨Ù‡ XSS Ø¢Ø³ÛŒØ¨â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª.")

        if vulnerabilities:
            self.log_issue("\n".join(vulnerabilities))
        else:
            print("âœ… Ø§Ù…Ù†ÛŒØª Ø³Ø§ÛŒØª Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³Øª.")

    def log_issue(self, message):
        """ Ø«Ø¨Øª Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø± Ù„Ø§Ú¯ Ùˆ Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ """
        logging.info(message)
        self.issue_log.append(message)
        print(f"âš ï¸ {message}")

    def backup_database(self):
        """ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø³Ø§ÛŒØª """
        print("ğŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ú¯Ø±ÙØªÙ† Ø¨Ú©Ø§Ù¾ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")
        os.system(f"mysqldump -u root -p database_name > {DB_BACKUP_PATH}")
        logging.info("âœ… Ø¨Ú©Ø§Ù¾ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    def monitor_cpu_usage(self):
        """ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ù…ØµØ±Ù CPU Ùˆ RAM Ø³Ø±ÙˆØ± """
        cpu_usage = psutil.cpu_percent(interval=1)
        ram_usage = psutil.virtual_memory().percent

        if cpu_usage > 85:
            self.log_issue(f"ğŸ”¥ Ù‡Ø´Ø¯Ø§Ø±: Ù…ØµØ±Ù CPU Ø¨Ø§Ù„Ø§Ø³Øª ({cpu_usage}%)")
        if ram_usage > 85:
            self.log_issue(f"âš¡ Ù‡Ø´Ø¯Ø§Ø±: Ù…ØµØ±Ù RAM Ø¨Ø§Ù„Ø§Ø³Øª ({ram_usage}%)")

    def optimize_website(self):
        """ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§ÛŒØª Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ """
        print("âš¡ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© Ø³Ø§ÛŒØª...")
        os.system("find /var/www/html -type f -name '*.css' -exec gzip {} \;")
        os.system("find /var/www/html -type f -name '*.js' -exec gzip {} \;")
        logging.info("âœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    def analyze_user_behavior(self):
        """ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† """
        print("ğŸ“Š Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†...")
        data = np.array([
            [10, 5], [20, 10], [15, 7], [25, 12], [30, 15]
        ])
        labels = [0, 1, 0, 1, 1]

        model = RandomForestClassifier()
        model.fit(data, labels)

        prediction = model.predict([[18, 8]])  # Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        print(f"ğŸ”® Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {'ÙØ¹Ø§Ù„ÛŒØª Ø¨Ø§Ù„Ø§' if prediction[0] else 'ÙØ¹Ø§Ù„ÛŒØª Ù¾Ø§ÛŒÛŒÙ†'}")

    def apply_updates(self):
        """ Ù†ØµØ¨ Ùˆ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø² Ø³Ø§ÛŒØª """
        print("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù†ØµØ¨ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯...")
        os.system("apt update && apt upgrade -y")
        logging.info("âœ… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    def run_all_tasks(self):
        """ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ù‡ ÙˆØ¸Ø§ÛŒÙ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø³Ø§ÛŒØª """
        self.check_site_status()
        self.analyze_html()
        self.check_security()
        self.monitor_cpu_usage()
        self.optimize_website()
        self.backup_database()
        self.analyze_user_behavior()
        self.apply_updates()

# Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
site_ai = SmartSiteAI(SITE_URL)
site_ai.run_all_tasks()
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/analyze', methods=['GET'])
def analyze():
    url = request.args.get('url')
    if not url:
        return jsonify({"error": "No URL provided"}), 400
    return jsonify({"message": f"Analyzing {url}..."}), 200

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, jsonify, request

app = Flask(__name__)

# Endpoint for analyzing gaming news
@app.route('/analyze', methods=['GET'])
def analyze():
    url = request.args.get('url')
    if not url:
        return jsonify({"error": "No URL provided"}), 400
    # Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯ÛŒÙ…ÛŒÙ†Ú¯ Ùˆ Ø§Ø®Ø¨Ø§Ø± Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ
    # Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„: Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø± Ú¯ÛŒÙ…ÛŒÙ†Ú¯ Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ.
    return jsonify({"message": f"Analyzing gaming news at {url}..."}), 200

if __name__ == '__main__':
    app.run(debug=True)
from flask import Flask, jsonify, request
import openai
import requests
from bs4 import BeautifulSoup

app = Flask(__name__)

# API key for OpenAI GPT-3
openai.api_key = 'your_openai_api_key'  # Replace with your actual API key

@app.route('/analyze', methods=['GET'])
def analyze_url():
    url = request.args.get('url')
    if not url:
        return jsonify({"error": "No URL provided"}), 400

    try:
        # Request the content of the URL
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        article_text = soup.get_text()

        # Use OpenAI API to analyze the content
        analysis = openai.Completion.create(
            engine="text-davinci-003",  # Replace with your model
            prompt=f"Analyze the following gaming news article and provide a summary: {article_text}",
            max_tokens=500
        )

        return jsonify({"message": analysis.choices[0].text.strip()}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
@app.route('/')
def home():
    return render_template('index.html')
app.debug = False
